# ============================================================================
# EMBEDDING MODEL FINE-TUNING CONFIGURATION
# ============================================================================
# This config controls all aspects of fine-tuning a Norwegian embedding model
# using triplet data (anchor, positive, negative sentences).
#
# WHAT IS AN EMBEDDING MODEL?
# An embedding model converts text into numerical vectors (embeddings) where
# semantically similar texts have similar vectors. This is crucial for tasks
# like semantic search, clustering, and retrieval.
#
# WHAT IS TRIPLET TRAINING?
# - Anchor: A reference sentence
# - Positive: A sentence similar to the anchor (should be close in vector space)
# - Negative: A sentence different from the anchor (should be far in vector space)
# The model learns to pull positives closer and push negatives farther away.
# ============================================================================

# MODEL CONFIGURATION
# ----------------------------------------------------------------------------
model:
  # The base model to fine-tune. Using NorBERT4 - the Norwegian equivalent of ModernBERT.
  # NorBERT4 has modern architecture (FlashAttention2, long context) and was proven by Fremtind
  # to achieve 94.7% accuracy on this exact dataset.
  name: "ltg/norbert4-base"  # 149M params - v1 achieved 95.52% accuracy, now testing v2 with optimized LR
  # name: "ltg/norbert4-large"  # 360M params - 2.42x larger, will train after validating LR on base
  # name: "ltg/norbert3-base"  # Alternative: Older architecture, no proven embedding examples
  # name: "NbAiLab/nb-bert-base"  # Alternative: 2021 tech, older training data

  # Maximum sequence length (in tokens). Longer sequences are truncated.
  # 128 is good for short sentences, 256-512 for longer documents.
  max_seq_length: 128

# DATASET CONFIGURATION
# ----------------------------------------------------------------------------
dataset:
  # HuggingFace dataset to use for training
  name: "Fremtind/all-nli-norwegian"

  # Dataset split configuration
  train_split: "train"      # Split used for training (569K samples)
  eval_split: "dev"         # Split used for evaluation during training
  test_split: "test"        # Split used for final testing

  # Column names in the dataset (Fremtind dataset uses these names)
  anchor_column: "anchor"      # The reference sentence
  positive_column: "positive"  # The similar sentence
  negative_column: "negative"  # The dissimilar sentence

  # Optional: Use a subset for faster experimentation
  # Set to null to use the full dataset
  max_train_samples: null  # e.g., 10000 for quick testing
  max_eval_samples: null   # e.g., 1000 for quick testing

# TRAINING HYPERPARAMETERS
# ----------------------------------------------------------------------------
training:
  # OUTPUT DIRECTORY
  # Where to save model checkpoints and training logs
  output_dir: "models/norbert4-base-nli-norwegian-v2"

  # LOSS FUNCTION
  # MultipleNegativesRankingLoss is the best for triplet data:
  # - Treats other positives in the batch as additional negatives (efficient!)
  # - Works well with large batch sizes
  loss_function: "MultipleNegativesRankingLoss"

  # BATCH SIZE
  # Number of triplets processed at once
  # M4 Pro Max 64GB unified memory
  # v1 used 35 GB at batch_size=32 (55% utilization) - we have headroom!
  # Increasing to 48 for better MultipleNegativesRankingLoss performance (more in-batch negatives)
  # Estimated memory: ~52 GB at batch_size=48 (81% utilization)
  # Larger batches = better for MultipleNegativesRankingLoss (more negatives)
  # NorBERT4 with FlashAttention2 is more memory-efficient than NorBERT3
  per_device_train_batch_size: 48  # Increased from 32 - v1 only used 35GB/64GB
  per_device_eval_batch_size: 96   # Increased from 64

  # TRAINING DURATION
  # Number of times to go through the entire dataset
  # 1 epoch is often enough for large datasets (500K+ samples)
  num_train_epochs: 1

  # LEARNING RATE
  # How much to update weights in each step
  # Optimized based on LR sweet spot analysis of previous training run
  # Analysis found 1.15e-05 as the optimal max LR (previous: 2.0e-5)
  # This avoids overly aggressive restarts while still escaping local minima
  learning_rate: 1.15e-5

  # WARMUP
  # Gradually increase learning rate at the start
  # Helps stabilize training
  # Reduced from 0.1 to 0.05 based on LR analysis
  # Shorter warmup allows more time at optimal LR range
  warmup_ratio: 0.05

  # WEIGHT DECAY
  # Regularization to prevent overfitting
  # 0.01 is standard for transformer models
  weight_decay: 0.01

  # EVALUATION STRATEGY
  # When to evaluate the model during training
  # "steps" = evaluate every N steps, "epoch" = evaluate after each epoch
  eval_strategy: "steps"
  eval_steps: 500  # Evaluate every 500 training steps

  # SAVING STRATEGY
  # When to save model checkpoints
  save_strategy: "steps"
  save_steps: 500          # Save every 500 steps
  save_total_limit: 3      # Keep only the 3 best checkpoints (saves disk space)
  load_best_model_at_end: true  # Load the best checkpoint at the end

  # LOGGING
  # How often to log training metrics (loss, learning rate, etc.)
  logging_steps: 100  # Log every 100 steps
  logging_dir: "models/norbert4-base-nli-norwegian-v2/logs"

  # PERFORMANCE OPTIMIZATIONS
  # Mixed precision training = faster training, less memory
  # bfloat16 is better than fp16 for training stability (wider dynamic range)
  # NorBERT4 with FlashAttention2 benefits from mixed precision
  fp16: false  # MPS doesn't support fp16 autocast
  bf16: true   # Use bfloat16 instead - supported on MPS and more stable!

  # Gradient accumulation = simulate larger batch sizes
  # Effective batch size = per_device_batch_size * gradient_accumulation_steps
  gradient_accumulation_steps: 1

  # REPRODUCIBILITY
  # Random seed for reproducible results
  seed: 42

  # DATA LOADING
  # Number of CPU workers for loading data
  # More workers = faster data loading (but uses more CPU)
  # NOTE: Automatically set to 0 on Apple Silicon (MPS) to avoid multiprocessing errors
  dataloader_num_workers: 4

# EVALUATION CONFIGURATION
# ----------------------------------------------------------------------------
evaluation:
  # Metric to use for selecting the best model
  # For embeddings, we typically use the evaluator's main metric
  metric_for_best_model: "eval_cosine_accuracy"

  # Whether higher metric values are better
  greater_is_better: true

  # Batch size for evaluation
  # Can be larger than training batch size since we don't need gradients
  batch_size: 64

# EXPERIMENT TRACKING
# ----------------------------------------------------------------------------
experiment_tracking:
  # MLflow experiment tracking
  use_mlflow: true
  mlflow_experiment_name: "norwegian-embedding-training"
  mlflow_run_name: null  # Auto-generated from model name if not set
  mlflow_tracking_uri: null  # Uses local ./mlruns if not set
  mlflow_tags:
    model_type: "embedding"
    language: "norwegian"
    dataset: "all-nli-norwegian"

  # TensorBoard logging (automatically enabled if tensorboard installed)
  # Logs are saved to logging_dir specified in training section
  # View with: tensorboard --logdir models/norbert4-base-nli-norwegian-v2/logs
  # Or start MLflow UI: mlflow ui --backend-store-uri ./mlruns

  # Report to (HuggingFace Trainer built-in)
  # Options: "tensorboard", "wandb", "mlflow", "all", "none"
  report_to: ["tensorboard", "mlflow"]  # Will use both for local tracking

# LEARNING RATE SCHEDULER
# ----------------------------------------------------------------------------
lr_scheduler:
  # Scheduler type
  # Options: "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"
  type: "cosine_with_restarts"  # Cosine annealing with warm restarts

  # Cosine with restarts specific parameters
  # Optimized based on LR analysis: 12 cycles matches the ~1,441 step stagnation intervals
  # Each cycle will:
  #   - Start at max LR (1.15e-05) after restart
  #   - Decay to min LR (2.67e-07) using cosine schedule
  #   - Restart to help escape local minima
  # This configuration pulls the model out of plateaus without being too aggressive
  num_cycles: 12  # Increased from 3 based on stagnation analysis (for cosine_with_restarts)

  # Note: warmup_ratio is already defined in training section above

# ADVANCED OPTIONS (Optional)
# ----------------------------------------------------------------------------
advanced:
  # Push model to HuggingFace Hub periodically during training
  push_to_hub: true
  hub_model_id: "thivy/norbert4-base-nli-norwegian-v2"  # v2 with optimized LR scheduler
  hub_token: null     # Will use HF_TOKEN environment variable or huggingface-cli login
  hub_strategy: "checkpoint"  # Push every time we save a checkpoint (every 500 steps)
  hub_private_repo: false  # Set to true for private repository

  # Resume training from a checkpoint
  resume_from_checkpoint: null  # Path to checkpoint directory

# COMPUTE CONFIGURATION
# ----------------------------------------------------------------------------
compute:
  # Device to use for training
  # Options: "auto" (automatic detection), "cuda" (NVIDIA GPU), "mps" (Apple Silicon), "cpu"
  device: "auto"
